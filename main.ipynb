{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Model page: https://huggingface.co/google-bert/bert-base-uncased\n",
    "\n",
    "⚠️ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/google-bert/bert-base-uncased)\n",
    "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) 🙏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2025-05-27T12:19:10.413436Z",
     "iopub.status.busy": "2025-05-27T12:19:10.412805Z",
     "iopub.status.idle": "2025-05-27T12:19:23.270417Z",
     "shell.execute_reply": "2025-05-27T12:19:23.269530Z",
     "shell.execute_reply.started": "2025-05-27T12:19:10.413412Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading transformers-4.52.3-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.3\n",
      "    Uninstalling transformers-4.51.3:\n",
      "      Successfully uninstalled transformers-4.51.3\n",
      "Successfully installed transformers-4.52.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:22:06.560670Z",
     "iopub.status.busy": "2025-05-27T12:22:06.559959Z",
     "iopub.status.idle": "2025-05-27T12:22:06.566322Z",
     "shell.execute_reply": "2025-05-27T12:22:06.565524Z",
     "shell.execute_reply.started": "2025-05-27T12:22:06.560640Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is ready? cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'GPU is ready? {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:19:51.648047Z",
     "iopub.status.busy": "2025-05-27T12:19:51.647409Z",
     "iopub.status.idle": "2025-05-27T12:19:53.824741Z",
     "shell.execute_reply": "2025-05-27T12:19:53.824036Z",
     "shell.execute_reply.started": "2025-05-27T12:19:51.648027Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>is_depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dear american teens question dutch person hear...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nothing look forward lifei dont many reasons k...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>music recommendations im looking expand playli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im done trying feel betterthe reason im still ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worried  year old girl subject domestic physic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35703</th>\n",
       "      <td>is that snow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35704</th>\n",
       "      <td>moulin rouge mad me cry once again</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35705</th>\n",
       "      <td>trying to shout but can t find people on the list</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35706</th>\n",
       "      <td>ughh can t find my red sox hat got ta wear thi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35707</th>\n",
       "      <td>slept wonderfully finally tried swatching for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35708 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  is_depression\n",
       "0      dear american teens question dutch person hear...              0\n",
       "1      nothing look forward lifei dont many reasons k...              1\n",
       "2      music recommendations im looking expand playli...              0\n",
       "3      im done trying feel betterthe reason im still ...              1\n",
       "4      worried  year old girl subject domestic physic...              1\n",
       "...                                                  ...            ...\n",
       "35703                                       is that snow              0\n",
       "35704                 moulin rouge mad me cry once again              0\n",
       "35705  trying to shout but can t find people on the list              0\n",
       "35706  ughh can t find my red sox hat got ta wear thi...              0\n",
       "35707  slept wonderfully finally tried swatching for ...              0\n",
       "\n",
       "[35708 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"depression-dataset-combined.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:19:53.826508Z",
     "iopub.status.busy": "2025-05-27T12:19:53.826276Z",
     "iopub.status.idle": "2025-05-27T12:19:53.836878Z",
     "shell.execute_reply": "2025-05-27T12:19:53.836296Z",
     "shell.execute_reply.started": "2025-05-27T12:19:53.826490Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_depression\n",
       "0    18039\n",
       "1    17669\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_depression'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label Indicator**\n",
    "- `0` indicating non-depression\n",
    "- `1` indicating depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:19:53.837816Z",
     "iopub.status.busy": "2025-05-27T12:19:53.837572Z",
     "iopub.status.idle": "2025-05-27T12:19:53.969911Z",
     "shell.execute_reply": "2025-05-27T12:19:53.969288Z",
     "shell.execute_reply.started": "2025-05-27T12:19:53.837799Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'is_depression'],\n",
       "        num_rows: 28566\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'is_depression'],\n",
       "        num_rows: 7142\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# reset index\n",
    "train = train.reset_index()[['text', 'is_depression']]\n",
    "test = test.reset_index()[['text', 'is_depression']]\n",
    "\n",
    "# dataframes to datadict --> nyesuaiin format huggingface\n",
    "train_ds = Dataset.from_pandas(train)\n",
    "test_ds = Dataset.from_pandas(test)\n",
    "\n",
    "dataset = DatasetDict()\n",
    "dataset['train'] = train_ds\n",
    "dataset['test'] = test_ds\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:22:09.661549Z",
     "iopub.status.busy": "2025-05-27T12:22:09.660812Z",
     "iopub.status.idle": "2025-05-27T12:22:25.548632Z",
     "shell.execute_reply": "2025-05-27T12:22:25.547908Z",
     "shell.execute_reply.started": "2025-05-27T12:22:09.661515Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446e6fee41734a3a8c324b20c477d44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28566 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389bac6c761648d59394bc0f19ae3116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7142 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 28566\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 7142\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "# tokenize the whole dataset\n",
    "dataset_encoded = dataset.map(tokenize, batched=True, batch_size=None)\n",
    "\n",
    "# Rename 'is_depression' to 'labels' for the model\n",
    "dataset_encoded = dataset_encoded.rename_column('is_depression', 'labels')\n",
    "dataset_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2025-05-27T12:22:32.101362Z",
     "iopub.status.busy": "2025-05-27T12:22:32.100754Z",
     "iopub.status.idle": "2025-05-27T12:22:35.503383Z",
     "shell.execute_reply": "2025-05-27T12:22:35.502636Z",
     "shell.execute_reply.started": "2025-05-27T12:22:32.101329Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676e7a12a3eb41d79d00ba91517399f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\", num_labels=2)\n",
    "model.to(device) # set device to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:22:38.333470Z",
     "iopub.status.busy": "2025-05-27T12:22:38.333169Z",
     "iopub.status.idle": "2025-05-27T12:22:38.338298Z",
     "shell.execute_reply": "2025-05-27T12:22:38.337541Z",
     "shell.execute_reply.started": "2025-05-27T12:22:38.333448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(preds):\n",
    "    labels = preds.label_ids\n",
    "    predictions = preds.predictions.argmax(-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy-score\": accuracy,\n",
    "        \"precision-score\": precision,\n",
    "        \"recall-score\": recall,\n",
    "        \"f1-score\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:24:59.339307Z",
     "iopub.status.busy": "2025-05-27T12:24:59.338983Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c86325d37fd403a89891cebbc46c184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6933, 'grad_norm': 3.8943581581115723, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}\n",
      "{'loss': 0.3948, 'grad_norm': 20.534740447998047, 'learning_rate': 4.46e-05, 'epoch': 0.12}\n",
      "{'loss': 0.3948, 'grad_norm': 20.534740447998047, 'learning_rate': 4.46e-05, 'epoch': 0.12}\n",
      "{'loss': 0.2895, 'grad_norm': 0.29641634225845337, 'learning_rate': 4.8080877313228236e-05, 'epoch': 0.25}\n",
      "{'loss': 0.2895, 'grad_norm': 0.29641634225845337, 'learning_rate': 4.8080877313228236e-05, 'epoch': 0.25}\n",
      "{'loss': 0.2305, 'grad_norm': 20.884241104125977, 'learning_rate': 4.589738568491139e-05, 'epoch': 0.37}\n",
      "{'loss': 0.2305, 'grad_norm': 20.884241104125977, 'learning_rate': 4.589738568491139e-05, 'epoch': 0.37}\n",
      "{'loss': 0.2356, 'grad_norm': 6.044149398803711, 'learning_rate': 4.371389405659454e-05, 'epoch': 0.5}\n",
      "{'loss': 0.2356, 'grad_norm': 6.044149398803711, 'learning_rate': 4.371389405659454e-05, 'epoch': 0.5}\n",
      "{'loss': 0.2369, 'grad_norm': 4.318938732147217, 'learning_rate': 4.153040242827769e-05, 'epoch': 0.62}\n",
      "{'loss': 0.2369, 'grad_norm': 4.318938732147217, 'learning_rate': 4.153040242827769e-05, 'epoch': 0.62}\n",
      "{'loss': 0.2047, 'grad_norm': 0.22519953548908234, 'learning_rate': 3.9346910799960834e-05, 'epoch': 0.75}\n",
      "{'loss': 0.2047, 'grad_norm': 0.22519953548908234, 'learning_rate': 3.9346910799960834e-05, 'epoch': 0.75}\n",
      "{'loss': 0.2216, 'grad_norm': 4.1535234451293945, 'learning_rate': 3.7163419171643986e-05, 'epoch': 0.87}\n",
      "{'loss': 0.2216, 'grad_norm': 4.1535234451293945, 'learning_rate': 3.7163419171643986e-05, 'epoch': 0.87}\n",
      "{'loss': 0.1923, 'grad_norm': 0.24659746885299683, 'learning_rate': 3.497992754332714e-05, 'epoch': 1.0}\n",
      "{'loss': 0.1923, 'grad_norm': 0.24659746885299683, 'learning_rate': 3.497992754332714e-05, 'epoch': 1.0}\n",
      "{'loss': 0.152, 'grad_norm': 4.065303802490234, 'learning_rate': 3.279643591501028e-05, 'epoch': 1.12}\n",
      "{'loss': 0.152, 'grad_norm': 4.065303802490234, 'learning_rate': 3.279643591501028e-05, 'epoch': 1.12}\n",
      "{'loss': 0.1808, 'grad_norm': 0.2522638738155365, 'learning_rate': 3.061294428669343e-05, 'epoch': 1.25}\n",
      "{'loss': 0.1808, 'grad_norm': 0.2522638738155365, 'learning_rate': 3.061294428669343e-05, 'epoch': 1.25}\n",
      "{'loss': 0.1646, 'grad_norm': 4.260831356048584, 'learning_rate': 2.8429452658376584e-05, 'epoch': 1.37}\n",
      "{'loss': 0.1646, 'grad_norm': 4.260831356048584, 'learning_rate': 2.8429452658376584e-05, 'epoch': 1.37}\n",
      "{'loss': 0.147, 'grad_norm': 17.81580352783203, 'learning_rate': 2.6245961030059728e-05, 'epoch': 1.5}\n",
      "{'loss': 0.147, 'grad_norm': 17.81580352783203, 'learning_rate': 2.6245961030059728e-05, 'epoch': 1.5}\n",
      "{'loss': 0.1349, 'grad_norm': 0.25920578837394714, 'learning_rate': 2.406246940174288e-05, 'epoch': 1.62}\n",
      "{'loss': 0.1349, 'grad_norm': 0.25920578837394714, 'learning_rate': 2.406246940174288e-05, 'epoch': 1.62}\n",
      "{'loss': 0.1594, 'grad_norm': 0.28608235716819763, 'learning_rate': 2.1878977773426027e-05, 'epoch': 1.75}\n",
      "{'loss': 0.1594, 'grad_norm': 0.28608235716819763, 'learning_rate': 2.1878977773426027e-05, 'epoch': 1.75}\n",
      "{'loss': 0.1735, 'grad_norm': 0.11965873092412949, 'learning_rate': 1.9695486145109178e-05, 'epoch': 1.87}\n",
      "{'loss': 0.1735, 'grad_norm': 0.11965873092412949, 'learning_rate': 1.9695486145109178e-05, 'epoch': 1.87}\n",
      "{'loss': 0.1477, 'grad_norm': 3.6064281463623047, 'learning_rate': 1.7511994516792322e-05, 'epoch': 2.0}\n",
      "{'loss': 0.1477, 'grad_norm': 3.6064281463623047, 'learning_rate': 1.7511994516792322e-05, 'epoch': 2.0}\n",
      "{'loss': 0.1102, 'grad_norm': 0.06259805709123611, 'learning_rate': 1.5328502888475474e-05, 'epoch': 2.12}\n",
      "{'loss': 0.1102, 'grad_norm': 0.06259805709123611, 'learning_rate': 1.5328502888475474e-05, 'epoch': 2.12}\n",
      "{'loss': 0.1044, 'grad_norm': 1.631906509399414, 'learning_rate': 1.3145011260158621e-05, 'epoch': 2.25}\n",
      "{'loss': 0.1044, 'grad_norm': 1.631906509399414, 'learning_rate': 1.3145011260158621e-05, 'epoch': 2.25}\n",
      "{'loss': 0.1061, 'grad_norm': 8.233072280883789, 'learning_rate': 1.0961519631841771e-05, 'epoch': 2.37}\n",
      "{'loss': 0.1061, 'grad_norm': 8.233072280883789, 'learning_rate': 1.0961519631841771e-05, 'epoch': 2.37}\n",
      "{'loss': 0.102, 'grad_norm': 0.04991637542843819, 'learning_rate': 8.77802800352492e-06, 'epoch': 2.5}\n",
      "{'loss': 0.102, 'grad_norm': 0.04991637542843819, 'learning_rate': 8.77802800352492e-06, 'epoch': 2.5}\n",
      "{'loss': 0.1088, 'grad_norm': 0.06842633336782455, 'learning_rate': 6.594536375208068e-06, 'epoch': 2.62}\n",
      "{'loss': 0.1088, 'grad_norm': 0.06842633336782455, 'learning_rate': 6.594536375208068e-06, 'epoch': 2.62}\n",
      "{'loss': 0.0948, 'grad_norm': 0.061221908777952194, 'learning_rate': 4.411044746891217e-06, 'epoch': 2.75}\n",
      "{'loss': 0.0948, 'grad_norm': 0.061221908777952194, 'learning_rate': 4.411044746891217e-06, 'epoch': 2.75}\n",
      "{'loss': 0.0924, 'grad_norm': 0.055601611733436584, 'learning_rate': 2.2275531185743663e-06, 'epoch': 2.87}\n",
      "{'loss': 0.0924, 'grad_norm': 0.055601611733436584, 'learning_rate': 2.2275531185743663e-06, 'epoch': 2.87}\n",
      "{'loss': 0.0992, 'grad_norm': 4.404989242553711, 'learning_rate': 4.4061490257514934e-08, 'epoch': 3.0}\n",
      "{'loss': 0.0992, 'grad_norm': 4.404989242553711, 'learning_rate': 4.4061490257514934e-08, 'epoch': 3.0}\n",
      "{'train_runtime': 7404.7332, 'train_samples_per_second': 11.573, 'train_steps_per_second': 1.447, 'train_loss': 0.17014115288245804, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10713, training_loss=0.17014115288245804, metrics={'train_runtime': 7404.7332, 'train_samples_per_second': 11.573, 'train_steps_per_second': 1.447, 'total_flos': 2.254809122224128e+16, 'train_loss': 0.17014115288245804, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "logging_steps = len(dataset_encoded['train']) // batch_size\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'bert-base-uncased-depression-detector',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,  # lower batch size if memory issues occur\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=logging_steps,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model= model,\n",
    "    args= training_args,\n",
    "    train_dataset = dataset_encoded['train'],\n",
    "    eval_dataset = dataset_encoded['test'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('depression-bert-model\\\\tokenizer_config.json',\n",
       " 'depression-bert-model\\\\special_tokens_map.json',\n",
       " 'depression-bert-model\\\\vocab.txt',\n",
       " 'depression-bert-model\\\\added_tokens.json',\n",
       " 'depression-bert-model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('depression-bert-model')\n",
    "tokenizer.save_pretrained('depression-bert-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0d257818094e56950f6b44e15739c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/893 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.19086241722106934,\n",
       " 'eval_accuracy-score': 0.9581349761971436,\n",
       " 'eval_precision-score': 0.9557670772676372,\n",
       " 'eval_recall-score': 0.960337552742616,\n",
       " 'eval_f1-score': 0.9580468640381647,\n",
       " 'eval_runtime': 239.2396,\n",
       " 'eval_samples_per_second': 29.853,\n",
       " 'eval_steps_per_second': 3.733,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results = trainer.evaluate()\n",
    "evaluation_results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "vsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
